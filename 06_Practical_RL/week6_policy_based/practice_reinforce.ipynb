{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in TensorFlow\n",
    "\n",
    "This notebook implements a basic reinforce algorithm a.k.a. policy gradient for CartPole env.\n",
    "\n",
    "It has been deliberately written to be as simple and human-readable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook assumes that you have [openai gym](https://github.com/openai/gym) installed.\n",
    "\n",
    "In case you're running on a server, [use xvfb](https://github.com/openai/gym#rendering-on-a-server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efd5ab039b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efd69f66358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "#gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env,'env'):\n",
    "    env=env.env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the policy network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__. \n",
    "\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#create input variables. We only need <s,a,R> for REINFORCE\n",
    "states = tf.placeholder('float32',(None,)+state_dim,name=\"states\")\n",
    "actions = tf.placeholder('int32',name=\"action_ids\")\n",
    "cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#<define network graph using raw tf or any deep learning library>\n",
    "\n",
    "#logits = <linear outputs (symbolic) of your network>\n",
    "\n",
    "def build_mlp(\n",
    "        input_placeholder,\n",
    "        output_size,\n",
    "        n_layers=2,\n",
    "        size=64,\n",
    "        activation=tf.tanh,\n",
    "        output_activation=None\n",
    "        ):\n",
    "\n",
    "    output = input_placeholder\n",
    "    for i in range(n_layers):\n",
    "        output = tf.layers.dense(output, size, activation=activation,\n",
    "                use_bias=True, kernel_initializer=tf.orthogonal_initializer,\n",
    "                name=\"dense_{}\".format(i))\n",
    "    output = tf.layers.dense(output, output_size, activation=output_activation,\n",
    "            use_bias=True, kernel_initializer=tf.orthogonal_initializer,\n",
    "            name=\"dense_{}\".format(n_layers))\n",
    "\n",
    "    return output\n",
    "\n",
    "# Construct model\n",
    "logits = build_mlp(states, n_actions)\n",
    "policy = tf.nn.softmax(logits)\n",
    "negative_likelihoods = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=actions, logits=logits)\n",
    "weighted_negative_likelihoods = tf.multiply(negative_likelihoods, cumulative_rewards)\n",
    "loss = tf.reduce_mean(weighted_negative_likelihoods)\n",
    "\n",
    "#policy = tf.nn.softmax(logits)\n",
    "log_policy = tf.nn.log_softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility function to pick action in one given state\n",
    "get_action_proba = lambda s: policy.eval({states:[s]})[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "\n",
    "Following the REINFORCE algorithm, we can define our objective as follows: \n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get probabilities for parti\n",
    "#indices = tf.stack([tf.range(tf.shape(log_policy)[0]),actions],axis=-1)\n",
    "#log_policy_for_actions = tf.gather_nd(log_policy,indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy objective as in the last formula. please use mean, not sum.\n",
    "# note: you need to use log_policy_for_actions to get log probabilities for actions taken\n",
    "\n",
    "#J = <YOUR CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regularize with entropy\n",
    "#entropy = <compute entropy. Don't forget the sign!>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all network weights\n",
    "#all_weights = <a list of all trainable weights in your network>\n",
    "\n",
    "#weight updates. maximizing J is same as minimizing -J. Adding negative entropy.\n",
    "#loss = -J -0.1 * entropy\n",
    "\n",
    "#update = tf.train.AdamOptimizer().minimize(loss,var_list=all_weights)\n",
    "update = tf.train.AdamOptimizer(5e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards, #rewards at each step\n",
    "                           gamma = 0.99 #discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session \n",
    "    compute cumulative rewards R(s,a) (a.k.a. G(s,a) in Sutton '16)\n",
    "    R_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "    \n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute R_t = r_t + gamma*R_{t+1} recurrently\n",
    "    \n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "#    <your code here>\n",
    "        \n",
    "#    return <array of cumulative rewards>\n",
    "\n",
    "    cum_rewards = np.zeros_like(rewards, dtype=np.float32)\n",
    "    for i in range(len(cum_rewards)):\n",
    "        G = 0\n",
    "        for j in range(i, len(rewards)):\n",
    "            G += np.power(gamma, j-i) * rewards[j]\n",
    "        cum_rewards[i] = G\n",
    "        \n",
    "    return cum_rewards\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "assert len(get_cumulative_rewards(range(100))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,0,0,1,0],gamma=0.9),[1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,-2,3,-4,0],gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,2,3,4,0],gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(_states,_actions,_rewards):\n",
    "    \"\"\"given full session, trains agent with policy gradient\"\"\"\n",
    "    _cumulative_rewards = get_cumulative_rewards(_rewards)\n",
    "    update.run({states:_states,actions:_actions,cumulative_rewards:_cumulative_rewards})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with REINFORCE agent and train at the session end\"\"\"\n",
    "    \n",
    "    #arrays to record session\n",
    "    states,actions,rewards = [],[],[]\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #action probabilities array aka pi(a|s)\n",
    "        action_probas = get_action_proba(s)\n",
    "        \n",
    "        # a = <pick random action using action_probas>\n",
    "        a = np.random.choice(n_actions, 1, p=action_probas)[0]\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "            \n",
    "    train_step(states,actions,rewards)\n",
    "            \n",
    "    return sum(rewards)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:11.660\n",
      "mean reward:46.860\n",
      "mean reward:16.210\n",
      "mean reward:16.610\n",
      "mean reward:35.300\n",
      "mean reward:69.620\n",
      "mean reward:96.130\n",
      "mean reward:9.470\n",
      "mean reward:9.420\n",
      "mean reward:9.460\n",
      "mean reward:9.320\n",
      "mean reward:9.440\n",
      "mean reward:9.290\n",
      "mean reward:9.360\n",
      "mean reward:9.350\n",
      "mean reward:9.320\n",
      "mean reward:9.350\n",
      "mean reward:9.430\n",
      "mean reward:9.260\n",
      "mean reward:9.370\n",
      "mean reward:9.270\n",
      "mean reward:9.380\n",
      "mean reward:9.410\n",
      "mean reward:9.480\n",
      "mean reward:9.370\n",
      "mean reward:9.170\n",
      "mean reward:9.320\n",
      "mean reward:9.370\n",
      "mean reward:9.500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6d0543107f64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#generate new sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"mean reward:%.3f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-6d0543107f64>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#generate new sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"mean reward:%.3f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-7331f1025c76>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(t_max)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#action probabilities array aka pi(a|s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0maction_probas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_action_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# a = <pick random action using action_probas>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c5203abcffb5>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#utility function to pick action in one given state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_action_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/dl4cv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \"\"\"\n\u001b[0;32m--> 541\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl4cv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4083\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4084\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 4085\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl4cv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl4cv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl4cv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl4cv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dl4cv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "s = tf.InteractiveSession()\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    rewards = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    \n",
    "    print (\"mean reward:%.3f\"%(np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submit import submit_cartpole\n",
    "submit_cartpole(generate_session, <EMAIL>, <TOKEN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's all, thank you for your attention!\n",
    "# Not having enough? There's an actor-critic waiting for you in the honor section.\n",
    "# But make sure you've seen the videos first."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
